{"ast":null,"code":"import { captureException } from '../../exports.js';\nimport { SPAN_STATUS_ERROR } from '../../tracing/spanstatus.js';\nimport { GEN_AI_RESPONSE_STREAMING_ATTRIBUTE, GEN_AI_RESPONSE_FINISH_REASONS_ATTRIBUTE, GEN_AI_RESPONSE_TEXT_ATTRIBUTE, GEN_AI_RESPONSE_TOOL_CALLS_ATTRIBUTE } from '../gen-ai-attributes.js';\nimport { RESPONSE_EVENT_TYPES } from './constants.js';\nimport { isChatCompletionChunk, isResponsesApiStreamEvent, setCommonResponseAttributes, setTokenUsageAttributes } from './utils.js';\n\n/**\n * State object used to accumulate information from a stream of OpenAI events/chunks.\n */\n\n/**\n * Processes tool calls from a chat completion chunk delta.\n * Follows the pattern: accumulate by index, then convert to array at the end.\n *\n * @param toolCalls - Array of tool calls from the delta.\n * @param state - The current streaming state to update.\n *\n *  @see https://platform.openai.com/docs/guides/function-calling#streaming\n */\nfunction processChatCompletionToolCalls(toolCalls, state) {\n  for (const toolCall of toolCalls) {\n    const index = toolCall.index;\n    if (index === undefined || !toolCall.function) continue;\n\n    // Initialize tool call if this is the first chunk for this index\n    if (!(index in state.chatCompletionToolCalls)) {\n      state.chatCompletionToolCalls[index] = {\n        ...toolCall,\n        function: {\n          name: toolCall.function.name,\n          arguments: toolCall.function.arguments || ''\n        }\n      };\n    } else {\n      // Accumulate function arguments from subsequent chunks\n      const existingToolCall = state.chatCompletionToolCalls[index];\n      if (toolCall.function.arguments && existingToolCall?.function) {\n        existingToolCall.function.arguments += toolCall.function.arguments;\n      }\n    }\n  }\n}\n\n/**\n * Processes a single OpenAI ChatCompletionChunk event, updating the streaming state.\n *\n * @param chunk - The ChatCompletionChunk event to process.\n * @param state - The current streaming state to update.\n * @param recordOutputs - Whether to record output text fragments.\n */\nfunction processChatCompletionChunk(chunk, state, recordOutputs) {\n  state.responseId = chunk.id ?? state.responseId;\n  state.responseModel = chunk.model ?? state.responseModel;\n  state.responseTimestamp = chunk.created ?? state.responseTimestamp;\n  if (chunk.usage) {\n    // For stream responses, the input tokens remain constant across all events in the stream.\n    // Output tokens, however, are only finalized in the last event.\n    // Since we can't guarantee that the last event will include usage data or even be a typed event,\n    // we update the output token values on every event that includes them.\n    // This ensures that output token usage is always set, even if the final event lacks it.\n    state.promptTokens = chunk.usage.prompt_tokens;\n    state.completionTokens = chunk.usage.completion_tokens;\n    state.totalTokens = chunk.usage.total_tokens;\n  }\n  for (const choice of chunk.choices ?? []) {\n    if (recordOutputs) {\n      if (choice.delta?.content) {\n        state.responseTexts.push(choice.delta.content);\n      }\n\n      // Handle tool calls from delta\n      if (choice.delta?.tool_calls) {\n        processChatCompletionToolCalls(choice.delta.tool_calls, state);\n      }\n    }\n    if (choice.finish_reason) {\n      state.finishReasons.push(choice.finish_reason);\n    }\n  }\n}\n\n/**\n * Processes a single OpenAI Responses API streaming event, updating the streaming state and span.\n *\n * @param streamEvent - The event to process (may be an error or unknown object).\n * @param state - The current streaming state to update.\n * @param recordOutputs - Whether to record output text fragments.\n * @param span - The span to update with error status if needed.\n */\nfunction processResponsesApiEvent(streamEvent, state, recordOutputs, span) {\n  if (!(streamEvent && typeof streamEvent === 'object')) {\n    state.eventTypes.push('unknown:non-object');\n    return;\n  }\n  if (streamEvent instanceof Error) {\n    span.setStatus({\n      code: SPAN_STATUS_ERROR,\n      message: 'internal_error'\n    });\n    captureException(streamEvent, {\n      mechanism: {\n        handled: false\n      }\n    });\n    return;\n  }\n  if (!('type' in streamEvent)) return;\n  const event = streamEvent;\n  if (!RESPONSE_EVENT_TYPES.includes(event.type)) {\n    state.eventTypes.push(event.type);\n    return;\n  }\n\n  // Handle output text delta\n  if (recordOutputs) {\n    // Handle tool call events for Responses API\n    if (event.type === 'response.output_item.done' && 'item' in event) {\n      state.responsesApiToolCalls.push(event.item);\n    }\n    if (event.type === 'response.output_text.delta' && 'delta' in event && event.delta) {\n      state.responseTexts.push(event.delta);\n      return;\n    }\n  }\n  if ('response' in event) {\n    const {\n      response\n    } = event;\n    state.responseId = response.id ?? state.responseId;\n    state.responseModel = response.model ?? state.responseModel;\n    state.responseTimestamp = response.created_at ?? state.responseTimestamp;\n    if (response.usage) {\n      // For stream responses, the input tokens remain constant across all events in the stream.\n      // Output tokens, however, are only finalized in the last event.\n      // Since we can't guarantee that the last event will include usage data or even be a typed event,\n      // we update the output token values on every event that includes them.\n      // This ensures that output token usage is always set, even if the final event lacks it.\n      state.promptTokens = response.usage.input_tokens;\n      state.completionTokens = response.usage.output_tokens;\n      state.totalTokens = response.usage.total_tokens;\n    }\n    if (response.status) {\n      state.finishReasons.push(response.status);\n    }\n    if (recordOutputs && response.output_text) {\n      state.responseTexts.push(response.output_text);\n    }\n  }\n}\n\n/**\n * Instruments a stream of OpenAI events, updating the provided span with relevant attributes and\n * optionally recording output text. This function yields each event from the input stream as it is processed.\n *\n * @template T - The type of events in the stream.\n * @param stream - The async iterable stream of events to instrument.\n * @param span - The span to add attributes to and to finish at the end of the stream.\n * @param recordOutputs - Whether to record output text fragments in the span.\n * @returns An async generator yielding each event from the input stream.\n */\nasync function* instrumentStream(stream, span, recordOutputs) {\n  const state = {\n    eventTypes: [],\n    responseTexts: [],\n    finishReasons: [],\n    responseId: '',\n    responseModel: '',\n    responseTimestamp: 0,\n    promptTokens: undefined,\n    completionTokens: undefined,\n    totalTokens: undefined,\n    chatCompletionToolCalls: {},\n    responsesApiToolCalls: []\n  };\n  try {\n    for await (const event of stream) {\n      if (isChatCompletionChunk(event)) {\n        processChatCompletionChunk(event, state, recordOutputs);\n      } else if (isResponsesApiStreamEvent(event)) {\n        processResponsesApiEvent(event, state, recordOutputs, span);\n      }\n      yield event;\n    }\n  } finally {\n    setCommonResponseAttributes(span, state.responseId, state.responseModel, state.responseTimestamp);\n    setTokenUsageAttributes(span, state.promptTokens, state.completionTokens, state.totalTokens);\n    span.setAttributes({\n      [GEN_AI_RESPONSE_STREAMING_ATTRIBUTE]: true\n    });\n    if (state.finishReasons.length) {\n      span.setAttributes({\n        [GEN_AI_RESPONSE_FINISH_REASONS_ATTRIBUTE]: JSON.stringify(state.finishReasons)\n      });\n    }\n    if (recordOutputs && state.responseTexts.length) {\n      span.setAttributes({\n        [GEN_AI_RESPONSE_TEXT_ATTRIBUTE]: state.responseTexts.join('')\n      });\n    }\n\n    // Set tool calls attribute if any were accumulated\n    const chatCompletionToolCallsArray = Object.values(state.chatCompletionToolCalls);\n    const allToolCalls = [...chatCompletionToolCallsArray, ...state.responsesApiToolCalls];\n    if (allToolCalls.length > 0) {\n      span.setAttributes({\n        [GEN_AI_RESPONSE_TOOL_CALLS_ATTRIBUTE]: JSON.stringify(allToolCalls)\n      });\n    }\n    span.end();\n  }\n}\nexport { instrumentStream };","map":{"version":3,"names":["processChatCompletionToolCalls","toolCalls","state","toolCall","index","undefined","function","chatCompletionToolCalls","name","arguments","existingToolCall","processChatCompletionChunk","chunk","recordOutputs","responseId","id","responseModel","model","responseTimestamp","created","usage","promptTokens","prompt_tokens","completionTokens","completion_tokens","totalTokens","total_tokens","choice","choices","delta","content","responseTexts","push","tool_calls","finish_reason","finishReasons","processResponsesApiEvent","streamEvent","span","eventTypes","Error","setStatus","code","SPAN_STATUS_ERROR","message","captureException","mechanism","handled","event","RESPONSE_EVENT_TYPES","includes","type","responsesApiToolCalls","item","response","created_at","input_tokens","output_tokens","status","output_text","instrumentStream","stream","isChatCompletionChunk","isResponsesApiStreamEvent","setCommonResponseAttributes","setTokenUsageAttributes","setAttributes","GEN_AI_RESPONSE_STREAMING_ATTRIBUTE","length","GEN_AI_RESPONSE_FINISH_REASONS_ATTRIBUTE","JSON","stringify","GEN_AI_RESPONSE_TEXT_ATTRIBUTE","join","chatCompletionToolCallsArray","Object","values","allToolCalls","GEN_AI_RESPONSE_TOOL_CALLS_ATTRIBUTE","end"],"sources":["/workspace/node_modules/@sentry/core/src/utils/openai/streaming.ts"],"sourcesContent":["import { captureException } from '../../exports';\nimport { SPAN_STATUS_ERROR } from '../../tracing';\nimport type { Span } from '../../types-hoist/span';\nimport {\n  GEN_AI_RESPONSE_FINISH_REASONS_ATTRIBUTE,\n  GEN_AI_RESPONSE_STREAMING_ATTRIBUTE,\n  GEN_AI_RESPONSE_TEXT_ATTRIBUTE,\n  GEN_AI_RESPONSE_TOOL_CALLS_ATTRIBUTE,\n} from '../gen-ai-attributes';\nimport { RESPONSE_EVENT_TYPES } from './constants';\nimport type { OpenAIResponseObject } from './types';\nimport {\n  type ChatCompletionChunk,\n  type ChatCompletionToolCall,\n  type ResponseFunctionCall,\n  type ResponseStreamingEvent,\n} from './types';\nimport {\n  isChatCompletionChunk,\n  isResponsesApiStreamEvent,\n  setCommonResponseAttributes,\n  setTokenUsageAttributes,\n} from './utils';\n\n/**\n * State object used to accumulate information from a stream of OpenAI events/chunks.\n */\ninterface StreamingState {\n  /** Types of events encountered in the stream. */\n  eventTypes: string[];\n  /** Collected response text fragments (for output recording). */\n  responseTexts: string[];\n  /** Reasons for finishing the response, as reported by the API. */\n  finishReasons: string[];\n  /** The response ID. */\n  responseId: string;\n  /** The model name. */\n  responseModel: string;\n  /** The timestamp of the response. */\n  responseTimestamp: number;\n  /** Number of prompt/input tokens used. */\n  promptTokens: number | undefined;\n  /** Number of completion/output tokens used. */\n  completionTokens: number | undefined;\n  /** Total number of tokens used (prompt + completion). */\n  totalTokens: number | undefined;\n  /**\n   * Accumulated tool calls from Chat Completion streaming, indexed by tool call index.\n   * @see https://platform.openai.com/docs/guides/function-calling?api-mode=chat#streaming\n   */\n  chatCompletionToolCalls: Record<number, ChatCompletionToolCall>;\n  /**\n   * Accumulated function calls from Responses API streaming.\n   * @see https://platform.openai.com/docs/guides/function-calling?api-mode=responses#streaming\n   */\n  responsesApiToolCalls: Array<ResponseFunctionCall | unknown>;\n}\n\n/**\n * Processes tool calls from a chat completion chunk delta.\n * Follows the pattern: accumulate by index, then convert to array at the end.\n *\n * @param toolCalls - Array of tool calls from the delta.\n * @param state - The current streaming state to update.\n *\n *  @see https://platform.openai.com/docs/guides/function-calling#streaming\n */\nfunction processChatCompletionToolCalls(toolCalls: ChatCompletionToolCall[], state: StreamingState): void {\n  for (const toolCall of toolCalls) {\n    const index = toolCall.index;\n    if (index === undefined || !toolCall.function) continue;\n\n    // Initialize tool call if this is the first chunk for this index\n    if (!(index in state.chatCompletionToolCalls)) {\n      state.chatCompletionToolCalls[index] = {\n        ...toolCall,\n        function: {\n          name: toolCall.function.name,\n          arguments: toolCall.function.arguments || '',\n        },\n      };\n    } else {\n      // Accumulate function arguments from subsequent chunks\n      const existingToolCall = state.chatCompletionToolCalls[index];\n      if (toolCall.function.arguments && existingToolCall?.function) {\n        existingToolCall.function.arguments += toolCall.function.arguments;\n      }\n    }\n  }\n}\n\n/**\n * Processes a single OpenAI ChatCompletionChunk event, updating the streaming state.\n *\n * @param chunk - The ChatCompletionChunk event to process.\n * @param state - The current streaming state to update.\n * @param recordOutputs - Whether to record output text fragments.\n */\nfunction processChatCompletionChunk(chunk: ChatCompletionChunk, state: StreamingState, recordOutputs: boolean): void {\n  state.responseId = chunk.id ?? state.responseId;\n  state.responseModel = chunk.model ?? state.responseModel;\n  state.responseTimestamp = chunk.created ?? state.responseTimestamp;\n\n  if (chunk.usage) {\n    // For stream responses, the input tokens remain constant across all events in the stream.\n    // Output tokens, however, are only finalized in the last event.\n    // Since we can't guarantee that the last event will include usage data or even be a typed event,\n    // we update the output token values on every event that includes them.\n    // This ensures that output token usage is always set, even if the final event lacks it.\n    state.promptTokens = chunk.usage.prompt_tokens;\n    state.completionTokens = chunk.usage.completion_tokens;\n    state.totalTokens = chunk.usage.total_tokens;\n  }\n\n  for (const choice of chunk.choices ?? []) {\n    if (recordOutputs) {\n      if (choice.delta?.content) {\n        state.responseTexts.push(choice.delta.content);\n      }\n\n      // Handle tool calls from delta\n      if (choice.delta?.tool_calls) {\n        processChatCompletionToolCalls(choice.delta.tool_calls, state);\n      }\n    }\n    if (choice.finish_reason) {\n      state.finishReasons.push(choice.finish_reason);\n    }\n  }\n}\n\n/**\n * Processes a single OpenAI Responses API streaming event, updating the streaming state and span.\n *\n * @param streamEvent - The event to process (may be an error or unknown object).\n * @param state - The current streaming state to update.\n * @param recordOutputs - Whether to record output text fragments.\n * @param span - The span to update with error status if needed.\n */\nfunction processResponsesApiEvent(\n  streamEvent: ResponseStreamingEvent | unknown | Error,\n  state: StreamingState,\n  recordOutputs: boolean,\n  span: Span,\n): void {\n  if (!(streamEvent && typeof streamEvent === 'object')) {\n    state.eventTypes.push('unknown:non-object');\n    return;\n  }\n  if (streamEvent instanceof Error) {\n    span.setStatus({ code: SPAN_STATUS_ERROR, message: 'internal_error' });\n    captureException(streamEvent, {\n      mechanism: {\n        handled: false,\n      },\n    });\n    return;\n  }\n\n  if (!('type' in streamEvent)) return;\n  const event = streamEvent as ResponseStreamingEvent;\n\n  if (!RESPONSE_EVENT_TYPES.includes(event.type)) {\n    state.eventTypes.push(event.type);\n    return;\n  }\n\n  // Handle output text delta\n  if (recordOutputs) {\n    // Handle tool call events for Responses API\n    if (event.type === 'response.output_item.done' && 'item' in event) {\n      state.responsesApiToolCalls.push(event.item);\n    }\n\n    if (event.type === 'response.output_text.delta' && 'delta' in event && event.delta) {\n      state.responseTexts.push(event.delta);\n      return;\n    }\n  }\n\n  if ('response' in event) {\n    const { response } = event as { response: OpenAIResponseObject };\n    state.responseId = response.id ?? state.responseId;\n    state.responseModel = response.model ?? state.responseModel;\n    state.responseTimestamp = response.created_at ?? state.responseTimestamp;\n\n    if (response.usage) {\n      // For stream responses, the input tokens remain constant across all events in the stream.\n      // Output tokens, however, are only finalized in the last event.\n      // Since we can't guarantee that the last event will include usage data or even be a typed event,\n      // we update the output token values on every event that includes them.\n      // This ensures that output token usage is always set, even if the final event lacks it.\n      state.promptTokens = response.usage.input_tokens;\n      state.completionTokens = response.usage.output_tokens;\n      state.totalTokens = response.usage.total_tokens;\n    }\n\n    if (response.status) {\n      state.finishReasons.push(response.status);\n    }\n\n    if (recordOutputs && response.output_text) {\n      state.responseTexts.push(response.output_text);\n    }\n  }\n}\n\n/**\n * Instruments a stream of OpenAI events, updating the provided span with relevant attributes and\n * optionally recording output text. This function yields each event from the input stream as it is processed.\n *\n * @template T - The type of events in the stream.\n * @param stream - The async iterable stream of events to instrument.\n * @param span - The span to add attributes to and to finish at the end of the stream.\n * @param recordOutputs - Whether to record output text fragments in the span.\n * @returns An async generator yielding each event from the input stream.\n */\nexport async function* instrumentStream<T>(\n  stream: AsyncIterable<T>,\n  span: Span,\n  recordOutputs: boolean,\n): AsyncGenerator<T, void, unknown> {\n  const state: StreamingState = {\n    eventTypes: [],\n    responseTexts: [],\n    finishReasons: [],\n    responseId: '',\n    responseModel: '',\n    responseTimestamp: 0,\n    promptTokens: undefined,\n    completionTokens: undefined,\n    totalTokens: undefined,\n    chatCompletionToolCalls: {},\n    responsesApiToolCalls: [],\n  };\n\n  try {\n    for await (const event of stream) {\n      if (isChatCompletionChunk(event)) {\n        processChatCompletionChunk(event as ChatCompletionChunk, state, recordOutputs);\n      } else if (isResponsesApiStreamEvent(event)) {\n        processResponsesApiEvent(event as ResponseStreamingEvent, state, recordOutputs, span);\n      }\n      yield event;\n    }\n  } finally {\n    setCommonResponseAttributes(span, state.responseId, state.responseModel, state.responseTimestamp);\n    setTokenUsageAttributes(span, state.promptTokens, state.completionTokens, state.totalTokens);\n\n    span.setAttributes({\n      [GEN_AI_RESPONSE_STREAMING_ATTRIBUTE]: true,\n    });\n\n    if (state.finishReasons.length) {\n      span.setAttributes({\n        [GEN_AI_RESPONSE_FINISH_REASONS_ATTRIBUTE]: JSON.stringify(state.finishReasons),\n      });\n    }\n\n    if (recordOutputs && state.responseTexts.length) {\n      span.setAttributes({\n        [GEN_AI_RESPONSE_TEXT_ATTRIBUTE]: state.responseTexts.join(''),\n      });\n    }\n\n    // Set tool calls attribute if any were accumulated\n    const chatCompletionToolCallsArray = Object.values(state.chatCompletionToolCalls);\n    const allToolCalls = [...chatCompletionToolCallsArray, ...state.responsesApiToolCalls];\n\n    if (allToolCalls.length > 0) {\n      span.setAttributes({\n        [GEN_AI_RESPONSE_TOOL_CALLS_ATTRIBUTE]: JSON.stringify(allToolCalls),\n      });\n    }\n\n    span.end();\n  }\n}\n"],"mappings":";;;;;;AAwBA;AACA;AACA;;AAgCA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAASA,8BAA8BA,CAACC,SAAS,EAA4BC,KAAK,EAAwB;EACxG,KAAK,MAAMC,QAAA,IAAYF,SAAS,EAAE;IAChC,MAAMG,KAAA,GAAQD,QAAQ,CAACC,KAAK;IAC5B,IAAIA,KAAA,KAAUC,SAAA,IAAa,CAACF,QAAQ,CAACG,QAAQ,EAAE;;IAEnD;IACI,IAAI,EAAEF,KAAA,IAASF,KAAK,CAACK,uBAAuB,CAAC,EAAE;MAC7CL,KAAK,CAACK,uBAAuB,CAACH,KAAK,IAAI;QACrC,GAAGD,QAAQ;QACXG,QAAQ,EAAE;UACRE,IAAI,EAAEL,QAAQ,CAACG,QAAQ,CAACE,IAAI;UAC5BC,SAAS,EAAEN,QAAQ,CAACG,QAAQ,CAACG,SAAA,IAAa;QACpD;MACA,CAAO;IACP,OAAW;MACX;MACM,MAAMC,gBAAA,GAAmBR,KAAK,CAACK,uBAAuB,CAACH,KAAK,CAAC;MAC7D,IAAID,QAAQ,CAACG,QAAQ,CAACG,SAAA,IAAaC,gBAAgB,EAAEJ,QAAQ,EAAE;QAC7DI,gBAAgB,CAACJ,QAAQ,CAACG,SAAA,IAAaN,QAAQ,CAACG,QAAQ,CAACG,SAAS;MAC1E;IACA;EACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAASE,0BAA0BA,CAACC,KAAK,EAAuBV,KAAK,EAAkBW,aAAa,EAAiB;EACnHX,KAAK,CAACY,UAAA,GAAaF,KAAK,CAACG,EAAA,IAAMb,KAAK,CAACY,UAAU;EAC/CZ,KAAK,CAACc,aAAA,GAAgBJ,KAAK,CAACK,KAAA,IAASf,KAAK,CAACc,aAAa;EACxDd,KAAK,CAACgB,iBAAA,GAAoBN,KAAK,CAACO,OAAA,IAAWjB,KAAK,CAACgB,iBAAiB;EAElE,IAAIN,KAAK,CAACQ,KAAK,EAAE;IACnB;IACA;IACA;IACA;IACA;IACIlB,KAAK,CAACmB,YAAA,GAAeT,KAAK,CAACQ,KAAK,CAACE,aAAa;IAC9CpB,KAAK,CAACqB,gBAAA,GAAmBX,KAAK,CAACQ,KAAK,CAACI,iBAAiB;IACtDtB,KAAK,CAACuB,WAAA,GAAcb,KAAK,CAACQ,KAAK,CAACM,YAAY;EAChD;EAEE,KAAK,MAAMC,MAAA,IAAUf,KAAK,CAACgB,OAAA,IAAW,EAAE,EAAE;IACxC,IAAIf,aAAa,EAAE;MACjB,IAAIc,MAAM,CAACE,KAAK,EAAEC,OAAO,EAAE;QACzB5B,KAAK,CAAC6B,aAAa,CAACC,IAAI,CAACL,MAAM,CAACE,KAAK,CAACC,OAAO,CAAC;MACtD;;MAEA;MACM,IAAIH,MAAM,CAACE,KAAK,EAAEI,UAAU,EAAE;QAC5BjC,8BAA8B,CAAC2B,MAAM,CAACE,KAAK,CAACI,UAAU,EAAE/B,KAAK,CAAC;MACtE;IACA;IACI,IAAIyB,MAAM,CAACO,aAAa,EAAE;MACxBhC,KAAK,CAACiC,aAAa,CAACH,IAAI,CAACL,MAAM,CAACO,aAAa,CAAC;IACpD;EACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,SAASE,wBAAwBA,CAC/BC,WAAW,EACXnC,KAAK,EACLW,aAAa,EACbyB,IAAI,EACE;EACN,IAAI,EAAED,WAAA,IAAe,OAAOA,WAAA,KAAgB,QAAQ,CAAC,EAAE;IACrDnC,KAAK,CAACqC,UAAU,CAACP,IAAI,CAAC,oBAAoB,CAAC;IAC3C;EACJ;EACE,IAAIK,WAAA,YAAuBG,KAAK,EAAE;IAChCF,IAAI,CAACG,SAAS,CAAC;MAAEC,IAAI,EAAEC,iBAAiB;MAAEC,OAAO,EAAE;IAAA,CAAkB,CAAC;IACtEC,gBAAgB,CAACR,WAAW,EAAE;MAC5BS,SAAS,EAAE;QACTC,OAAO,EAAE;MACjB;IACA,CAAK,CAAC;IACF;EACJ;EAEE,IAAI,EAAE,UAAUV,WAAW,CAAC,EAAE;EAC9B,MAAMW,KAAA,GAAQX,WAAA;EAEd,IAAI,CAACY,oBAAoB,CAACC,QAAQ,CAACF,KAAK,CAACG,IAAI,CAAC,EAAE;IAC9CjD,KAAK,CAACqC,UAAU,CAACP,IAAI,CAACgB,KAAK,CAACG,IAAI,CAAC;IACjC;EACJ;;EAEA;EACE,IAAItC,aAAa,EAAE;IACrB;IACI,IAAImC,KAAK,CAACG,IAAA,KAAS,+BAA+B,UAAUH,KAAK,EAAE;MACjE9C,KAAK,CAACkD,qBAAqB,CAACpB,IAAI,CAACgB,KAAK,CAACK,IAAI,CAAC;IAClD;IAEI,IAAIL,KAAK,CAACG,IAAA,KAAS,gCAAgC,WAAWH,KAAA,IAASA,KAAK,CAACnB,KAAK,EAAE;MAClF3B,KAAK,CAAC6B,aAAa,CAACC,IAAI,CAACgB,KAAK,CAACnB,KAAK,CAAC;MACrC;IACN;EACA;EAEE,IAAI,cAAcmB,KAAK,EAAE;IACvB,MAAM;MAAEM;IAAA,CAAS,GAAIN,KAAA;IACrB9C,KAAK,CAACY,UAAA,GAAawC,QAAQ,CAACvC,EAAA,IAAMb,KAAK,CAACY,UAAU;IAClDZ,KAAK,CAACc,aAAA,GAAgBsC,QAAQ,CAACrC,KAAA,IAASf,KAAK,CAACc,aAAa;IAC3Dd,KAAK,CAACgB,iBAAA,GAAoBoC,QAAQ,CAACC,UAAA,IAAcrD,KAAK,CAACgB,iBAAiB;IAExE,IAAIoC,QAAQ,CAAClC,KAAK,EAAE;MACxB;MACA;MACA;MACA;MACA;MACMlB,KAAK,CAACmB,YAAA,GAAeiC,QAAQ,CAAClC,KAAK,CAACoC,YAAY;MAChDtD,KAAK,CAACqB,gBAAA,GAAmB+B,QAAQ,CAAClC,KAAK,CAACqC,aAAa;MACrDvD,KAAK,CAACuB,WAAA,GAAc6B,QAAQ,CAAClC,KAAK,CAACM,YAAY;IACrD;IAEI,IAAI4B,QAAQ,CAACI,MAAM,EAAE;MACnBxD,KAAK,CAACiC,aAAa,CAACH,IAAI,CAACsB,QAAQ,CAACI,MAAM,CAAC;IAC/C;IAEI,IAAI7C,aAAA,IAAiByC,QAAQ,CAACK,WAAW,EAAE;MACzCzD,KAAK,CAAC6B,aAAa,CAACC,IAAI,CAACsB,QAAQ,CAACK,WAAW,CAAC;IACpD;EACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACO,gBAAgBC,gBAAgBA,CACrCC,MAAM,EACNvB,IAAI,EACJzB,aAAa,EACqB;EAClC,MAAMX,KAAK,GAAmB;IAC5BqC,UAAU,EAAE,EAAE;IACdR,aAAa,EAAE,EAAE;IACjBI,aAAa,EAAE,EAAE;IACjBrB,UAAU,EAAE,EAAE;IACdE,aAAa,EAAE,EAAE;IACjBE,iBAAiB,EAAE,CAAC;IACpBG,YAAY,EAAEhB,SAAS;IACvBkB,gBAAgB,EAAElB,SAAS;IAC3BoB,WAAW,EAAEpB,SAAS;IACtBE,uBAAuB,EAAE,EAAE;IAC3B6C,qBAAqB,EAAE;EAC3B,CAAG;EAED,IAAI;IACF,WAAW,MAAMJ,KAAA,IAASa,MAAM,EAAE;MAChC,IAAIC,qBAAqB,CAACd,KAAK,CAAC,EAAE;QAChCrC,0BAA0B,CAACqC,KAAA,EAA8B9C,KAAK,EAAEW,aAAa,CAAC;MACtF,CAAM,MAAO,IAAIkD,yBAAyB,CAACf,KAAK,CAAC,EAAE;QAC3CZ,wBAAwB,CAACY,KAAA,EAAiC9C,KAAK,EAAEW,aAAa,EAAEyB,IAAI,CAAC;MAC7F;MACM,MAAMU,KAAK;IACjB;EACA,UAAY;IACRgB,2BAA2B,CAAC1B,IAAI,EAAEpC,KAAK,CAACY,UAAU,EAAEZ,KAAK,CAACc,aAAa,EAAEd,KAAK,CAACgB,iBAAiB,CAAC;IACjG+C,uBAAuB,CAAC3B,IAAI,EAAEpC,KAAK,CAACmB,YAAY,EAAEnB,KAAK,CAACqB,gBAAgB,EAAErB,KAAK,CAACuB,WAAW,CAAC;IAE5Fa,IAAI,CAAC4B,aAAa,CAAC;MACjB,CAACC,mCAAmC,GAAG;IAC7C,CAAK,CAAC;IAEF,IAAIjE,KAAK,CAACiC,aAAa,CAACiC,MAAM,EAAE;MAC9B9B,IAAI,CAAC4B,aAAa,CAAC;QACjB,CAACG,wCAAwC,GAAGC,IAAI,CAACC,SAAS,CAACrE,KAAK,CAACiC,aAAa;MACtF,CAAO,CAAC;IACR;IAEI,IAAItB,aAAA,IAAiBX,KAAK,CAAC6B,aAAa,CAACqC,MAAM,EAAE;MAC/C9B,IAAI,CAAC4B,aAAa,CAAC;QACjB,CAACM,8BAA8B,GAAGtE,KAAK,CAAC6B,aAAa,CAAC0C,IAAI,CAAC,EAAE;MACrE,CAAO,CAAC;IACR;;IAEA;IACI,MAAMC,4BAAA,GAA+BC,MAAM,CAACC,MAAM,CAAC1E,KAAK,CAACK,uBAAuB,CAAC;IACjF,MAAMsE,YAAA,GAAe,CAAC,GAAGH,4BAA4B,EAAE,GAAGxE,KAAK,CAACkD,qBAAqB,CAAC;IAEtF,IAAIyB,YAAY,CAACT,MAAA,GAAS,CAAC,EAAE;MAC3B9B,IAAI,CAAC4B,aAAa,CAAC;QACjB,CAACY,oCAAoC,GAAGR,IAAI,CAACC,SAAS,CAACM,YAAY;MAC3E,CAAO,CAAC;IACR;IAEIvC,IAAI,CAACyC,GAAG,EAAE;EACd;AACA","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}